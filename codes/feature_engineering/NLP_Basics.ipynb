{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing With Python's NLTK Package"
      ],
      "metadata": {
        "id": "Q7T1L-mWjViw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started With Python’s NLTK"
      ],
      "metadata": {
        "id": "zJEoKSXXzB_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install NLTK**"
      ],
      "metadata": {
        "id": "VgVuD8aSayxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLeygzZXajss"
      },
      "outputs": [],
      "source": [
        "pip install nltk==3.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "oauqg5VjjWM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "8j1XHzivzSU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"\"\"You don't have to be French to enjoy a decent red wine,\" Charles Jousselin de Gruse used to tell his foreign guests whenever he entertained them in Paris. \"But you do have to be French to recognize one,\" he would add with a laugh. After a lifetime in the French diplomatic corps, the Count de Gruse lived with his wife in an elegant townhouse on Quai Voltaire. He was a likeable man, cultivated of course, with a well-deserved reputation as a generous host and an amusing raconteur.\"\"\""
      ],
      "metadata": {
        "id": "oVSe4DyfzVIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing by Sentence"
      ],
      "metadata": {
        "id": "l875a8zvF5Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences=sent_tokenize(example_string)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "dN9ToHd-F6d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in sentences:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "j8VD2uMGGvUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing by word"
      ],
      "metadata": {
        "id": "zGAJxfFENcng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words=word_tokenize(example_string)\n",
        "print(words)\n",
        "for w in words:\n",
        "  print(w)\n"
      ],
      "metadata": {
        "id": "fEZ3q5zYNdt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[]\n",
        "for s in sentences:\n",
        "  data.append(word_tokenize(s))\n",
        "print(data)"
      ],
      "metadata": {
        "id": "XywjsjTINtfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Stop Words"
      ],
      "metadata": {
        "id": "BsPAM96VPWk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "fSF2MQKyPXl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "nVfYpDB5PhAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "Z6mTj1AKLotQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "5F9p0-3eLzTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\""
      ],
      "metadata": {
        "id": "W7smgDPRPn2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(worf_quote)\n",
        "words_in_quote"
      ],
      "metadata": {
        "id": "PXCKPWd7PrE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "du8Q2qB1P0C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "g-lhkeMKP3Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\"\n",
        "words_in_quote = word_tokenize(worf_quote)\n",
        "words_in_quote"
      ],
      "metadata": {
        "id": "38mGJqmkS1IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "    if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)\n",
        "print(filtered_list)"
      ],
      "metadata": {
        "id": "EeU3PwdwQAKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove special charecters in filtered_list\n",
        "\n",
        "import re\n",
        "\n",
        "cleaned_list = []\n",
        "for word in filtered_list:\n",
        "  cleaned_word = re.sub(r'[^\\w\\s]', '', word) # Remove special characters\n",
        "  if cleaned_word: # Append only if the word is not empty after cleaning\n",
        "    cleaned_list.append(cleaned_word)\n",
        "\n",
        "print(cleaned_list)\n"
      ],
      "metadata": {
        "id": "ahkawagUBoVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "aWeUy4JFQ212"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# PorterStemmer is an algorithm"
      ],
      "metadata": {
        "id": "kj8NfoloQiT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()  #steamer is an object of porterstemmer"
      ],
      "metadata": {
        "id": "4lfQvacIQ_tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singleword= stemmer.stem(\"scarves\")\n",
        "singleword"
      ],
      "metadata": {
        "id": "YFh52BuiBIgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_stemming = \"\"\"\n",
        "The crew of the USS Discovery discovered many discoveries.\n",
        "Discovering is what explorers do.\"\"\""
      ],
      "metadata": {
        "id": "rseNs1eRRCdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_stemming)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "UNwLq4dcROK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "SJKDNJORRQoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizing"
      ],
      "metadata": {
        "id": "wgSfmikjRnou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lCw4qDotRm1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "hTF7HjR3RtQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "RrIGGuEORx_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "DslpUEPDSI31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))"
      ],
      "metadata": {
        "id": "1GZTsKeWIfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"scarves\"))"
      ],
      "metadata": {
        "id": "IleqF-SSTo6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_lemmatizing =\"\"\"\n",
        "The crew of the USS Discovery discovered many discoveries.\n",
        "Discovering is what explorers do.\"\"\""
      ],
      "metadata": {
        "id": "uy-zk1KaTl2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_lemmatizing)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "7XzvXYs0UNWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "id": "t5-j9uJFUZO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "id": "ivON5tihUqrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "id": "9-NbGr6UdWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"changed\")\n"
      ],
      "metadata": {
        "id": "yx6_mm3riR2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"changed\", pos=\"v\")"
      ],
      "metadata": {
        "id": "MU91AfApiwPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tagging Parts of Speech"
      ],
      "metadata": {
        "id": "yX2K3IaSmr8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sagan_quote = \"\"\"\n",
        "If you wish to make an apple pie from scratch,\n",
        "you must first invent the universe.\"\"\""
      ],
      "metadata": {
        "id": "DbGR5znlmrLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_sagan_quote = word_tokenize(sagan_quote)\n",
        "print(words_in_sagan_quote)"
      ],
      "metadata": {
        "id": "FO00H8y2m0W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "DgBaFUNSm8Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(words_in_sagan_quote)"
      ],
      "metadata": {
        "id": "iP8x_nKcm__-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "vnGNv2zEnf4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jabberwocky_excerpt = \"\"\"\n",
        "'Twas brillig, and the slithy toves did gyre and gimble in the wabe:\n",
        "all mimsy were the borogoves, and the mome raths outgrabe.\"\"\""
      ],
      "metadata": {
        "id": "7GPlbN4wni-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_excerpt = word_tokenize(jabberwocky_excerpt)"
      ],
      "metadata": {
        "id": "cx4dRIstn14a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(words_in_excerpt)"
      ],
      "metadata": {
        "id": "aBlPe_qEn7aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition"
      ],
      "metadata": {
        "id": "6eVdFTIKvGqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "nn-7ZkIYt3VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"F. Henly was born in San Francisco and he works at Microsoft.\"\n",
        "tokens = nltk.word_tokenize(sample_text)\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "print(tagged_tokens)"
      ],
      "metadata": {
        "id": "Si5EVxpHvEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "DvKmizM2shwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = nltk.ne_chunk(tagged_tokens)\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "AMSmqlpcvm22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy\n",
        "## Code Description\n",
        "\n",
        "This code uses the spaCy library to find variations of the term \"solar power\" in a given text.\n",
        "\n",
        "1. It imports the necessary modules and loads the English language model.\n",
        "2. It creates a Matcher object to define patterns for matching.\n",
        "3. Three patterns are specified:\n",
        "   - The exact term \"solarpower\" (all lowercase).\n",
        "   - The two words \"solar\" and \"power\" as separate tokens.\n",
        "   - The two words \"solar\" and \"power\" with a punctuation mark in between.\n",
        "4. These patterns are added to the matcher under the label `SolarPower`.\n",
        "5. The code processes a sample sentence and finds all occurrences of the specified patterns.\n",
        "6. Finally, it prints the matches found, showing their unique identifier and position in the text.\n"
      ],
      "metadata": {
        "id": "Ttaik5hRul3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIpGzbqLfQpM"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJUvCLRAh6_C"
      },
      "source": [
        "## Part of Speech Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PppzLgx7hAiN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Get the text\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(text)\n",
        "\n",
        "# Get the part-of-speech tags\n",
        "tags = [token.tag_ for token in doc]\n",
        "\n",
        "# Print the tags\n",
        "print(tags)\n",
        "\n",
        "# Visualize the part-of-speech tags\n",
        "displacy.render(doc, style='dep',jupyter=True, options={'distance': 90})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adAaQHiVjryp"
      },
      "source": [
        "##Named Entity Recogntion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggiDJKJDkBp7"
      },
      "outputs": [],
      "source": [
        "text = \"President Barack Obama gave a speech at the White House.\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Feature Extraction"
      ],
      "metadata": {
        "id": "HI1USWuBXqMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams"
      ],
      "metadata": {
        "id": "GEzOQCKGYYaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "ClKzhKXm7EaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "EeV6Ohb_gcmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"\"\"This is the first document.\",\n",
        "             \"This document is the second document.\",\n",
        "             \"And this is the third one.\",\n",
        "             \"Is this the first document\"\"\""
      ],
      "metadata": {
        "id": "NTArYVvp7US1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(example_string).ngrams(3)"
      ],
      "metadata": {
        "id": "lMXYtiY87gVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words(BOW) model"
      ],
      "metadata": {
        "id": "ymRYpdwM8WMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "E_bOsJPpwaNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tkxqdd4WwWvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"This is the first document.\",\n",
        "             \"This document is the second document.\",\n",
        "             \"And this is the third one.\",\n",
        "             \"Is this the first document?\",\n",
        "          'They love NLP but can not learn in two months'\n",
        "          'i love india and i hate terrorism',\n",
        "          'i love terrorism and i hate india']"
      ],
      "metadata": {
        "id": "Cdv4CZnDYznF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "3XmBj9-bY762"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer to the text corpus and transform it into a feature matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "-ynLeNmuZELR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names (n-grams)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Feature Names (n-grams):\")\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "Oj7x4uMrZLiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the feature matrix to an array for better readability\n",
        "feature_matrix = X.toarray()\n",
        "\n",
        "print(\"\\nFeature Matrix:\")\n",
        "print(feature_matrix)"
      ],
      "metadata": {
        "id": "TyStwTOEZSIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=feature_matrix,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "pja4FlC_9Nq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = ['They love NLP but can not learn in two months']\n",
        "vectorizer.transform(text2).toarray()"
      ],
      "metadata": {
        "id": "-hsYCYVj9mdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = [\"food was not bad\",\"I am not feeling bad\"]\n",
        "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
        "count_matrix = vectorizer.fit_transform(text)\n",
        "count_array = count_matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "As-PWJL-3Z6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term Frequency – Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "5Tfne-GqEqEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "37mUmpZwQYKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"i love NLP NLP\",\n",
        "        \"NLP NLP is the future\",\n",
        "        \"i will learn the NLP\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(text)\n",
        "count_array = matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ktdKqTFRWBF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "text =[\"This is the first document.\",\n",
        "        \"This document is the second document.\",\n",
        "        \"And this is the third one.\",\n",
        "        \"Is this the first document\"]\n",
        "'''\n",
        "'''\n",
        "text = [\"read  svm algorithm article dataaspirant blog\",\n",
        "        \"read randomforest algorithm article dataaspirant blog\"]\n",
        "'''\n",
        "text =[\"petrol cars  cheaper  diesel cars\",\n",
        "        \"diesel cheaper  petrol\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(text)\n",
        "count_array = matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "WKbpPqPkWkTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[\"petrol cars  are cheaper than diesel cars\",\n",
        "        \"diesel is cheaper  than petrol\"]"
      ],
      "metadata": {
        "id": "V434uxUu8Sf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings"
      ],
      "metadata": {
        "id": "T-1AFFxMyzkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "metadata": {
        "id": "0LlyhSVftOzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.3"
      ],
      "metadata": {
        "id": "nIBCraiAtuma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[d.split() for d in text]\n",
        "X"
      ],
      "metadata": {
        "id": "V9G4i64btWnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert text into the word2vec format.\n",
        "import gensim\n",
        "#from gensim.models import Word2Vec\n",
        "w2vecmodel=gensim.models.Word2Vec(sentences=X,vector_size=2,window=4,min_count=1)"
      ],
      "metadata": {
        "id": "GT6DkoWEt3qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can load the above word2vec file as a model."
      ],
      "metadata": {
        "id": "A0qC5s5auIFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2vecmodel)"
      ],
      "metadata": {
        "id": "zpo2o8hXuJCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(w2vecmodel.wv.index_to_key)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "LyfVNxtJuUgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "    ['this', 'is', 'the', 'first', 'document'],\n",
        "    ['this', 'document', 'is', 'the', 'second', 'document'],\n",
        "    ['and', 'this', 'is', 'the', 'third', 'one'],\n",
        "    ['is', 'this', 'the', 'first', 'document']\n",
        "]\n",
        "\n",
        "# Initialize the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=5, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train the model\n",
        "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "# Get vector for a word\n",
        "print(model.wv['document'])\n",
        "print(\"second = \",model.wv['second'])"
      ],
      "metadata": {
        "id": "u4MaffQ4bzSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USZBhD3IQpSt"
      },
      "outputs": [],
      "source": [
        "#convert text into the word2vec format.\n",
        "import gensim\n",
        "text = [\"I love, love, love the NLP\",\n",
        "        \"NLP is the future\",\n",
        "        \"I will learn the NLP\"]\n",
        "X=[d.split() for d in text]\n",
        "w2vecmodel=gensim.models.Word2Vec(sentences=X,vector_size=2,window=4,min_count=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7X5ZV3QpSu"
      },
      "source": [
        "Now, we can load the above word2vec file as a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuHBpap_QpSu"
      },
      "outputs": [],
      "source": [
        "print(w2vecmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pbKY0v6QpSu"
      },
      "outputs": [],
      "source": [
        "words = w2vecmodel.wv.index_to_key\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXBa49-rJM3h"
      },
      "outputs": [],
      "source": [
        "words = w2vecmodel.wv.key_to_index\n",
        "print(words.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXm4bkVQQpSu"
      },
      "outputs": [],
      "source": [
        "print(words.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pknB4PAUQpSv"
      },
      "outputs": [],
      "source": [
        "print(w2vecmodel.wv.vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH8Pb2TkP_Pw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y = w2vecmodel.wv.vectors[:,0]\n",
        "x = w2vecmodel.wv.vectors[:,1]\n",
        "labels = words.keys()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)\n",
        "\n",
        "for i, txt in enumerate(labels):\n",
        "    ax.annotate(txt, (x[i], y[i]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpWQc0Giwwb4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}