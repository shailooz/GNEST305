{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Synthetic Data**"
      ],
      "metadata": {
        "id": "N149pUuJddIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dictionary with the data\n",
        "data = {\n",
        "    'Numeric_Column_1': [10.5, 12.1, 9.8, np.nan, 11.3, 10.0, 13.5, 11.8, 9.5, 10.9,\n",
        "                         12.0, np.nan, 10.2, 11.5, 9.9, 13.0, 11.1, np.nan, 12.5, 10.7],\n",
        "    'Numeric_Column_2': [55, 62, np.nan, 58, 65, 50, 70, 61, 53, 59,\n",
        "                         63, 57, 60, np.nan, 54, 68, 56, 64, 66, 52],\n",
        "    'Categorical_Column_1': [1, 2, 1, 3, 2, 1, 3, 2, 1, 3,\n",
        "                             2, 1, 3, 2, 1, 3, 2, 1, 3, 2],\n",
        "    'Categorical_Column_2': [101, 102, 101, 103, 102, 101, 103, 102, 101, 103,\n",
        "                             102, 101, 103, np.nan, 101, 103, 102, 101, 103, 102]\n",
        "}\n"
      ],
      "metadata": {
        "id": "lNxodJrKI13j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some random missing values in categorical columns (represented by NaN)\n",
        "# For demonstrating missing value handling in categorical columns, we'll add more NaNs\n",
        "df.loc[[2, 13, 17], 'Categorical_Column_1'] = np.nan\n",
        "df.loc[[5, 9, 14], 'Categorical_Column_2'] = np.nan"
      ],
      "metadata": {
        "id": "reIgtV3FKs5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "D7GI59TcKNNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Missing Data Handling**"
      ],
      "metadata": {
        "id": "QSllwcu_ds-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform missing data handling in above dataset\n",
        "\n",
        "# Display initial info about missing values\n",
        "print(\"Initial missing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZrzlS22KYjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values:\n",
        "# For numerical columns, we can fill missing values with the mean or median.\n",
        "# Let's use the mean for 'Numeric_Column_1' and median for 'Numeric_Column_2'.\n",
        "df.fillna({'Numeric_Column_1':df['Numeric_Column_1'].mean()}, inplace=True)\n",
        "df.fillna({'Numeric_Column_2':df['Numeric_Column_2'].median()}, inplace=True)\n"
      ],
      "metadata": {
        "id": "btuREtDmKxsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For categorical columns, we can fill missing values with the mode (most frequent value).\n",
        "df.fillna({'Categorical_Column_1':df['Categorical_Column_1'].mode()[0]}, inplace=True)\n",
        "df.fillna({'Categorical_Column_2':df['Categorical_Column_2'].mode()[0]}, inplace=True)\n"
      ],
      "metadata": {
        "id": "EOwU3AT_K60I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display info about missing values after handling\n",
        "print(\"Missing values after handling:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "wgwwx9dZK8aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display the first few rows of the cleaned DataFrame\n",
        "print(\"DataFrame after handling missing values:\")\n",
        "df"
      ],
      "metadata": {
        "id": "WBohA5QqK-Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Duplicate a row (e.g., the first row) three times and append it\n",
        "duplicated_rows = pd.concat([df.iloc[[0]]] * 3, ignore_index=True)\n",
        "df = pd.concat([df, duplicated_rows], ignore_index=True)\n",
        "\n",
        "print(\"DataFrame after adding 3 duplicate rows:\")\n",
        "df\n"
      ],
      "metadata": {
        "id": "oM_MppyvM-N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data Cleaning**"
      ],
      "metadata": {
        "id": "9CXRChgJd4hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying and removing duplicate rows\n",
        "print(\"Initial number of rows:\", len(df))\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Number of rows after removing duplicates:\", len(df))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "rafzYSdYM1aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying outliers (example using IQR for a numerical column)\n",
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = df['Numeric_Column_1'].quantile(0.25)\n",
        "Q3 = df['Numeric_Column_1'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR"
      ],
      "metadata": {
        "id": "o9xAP88_NMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify outliers\n",
        "outliers = df[(df['Numeric_Column_1'] < lower_bound) | (df['Numeric_Column_1'] > upper_bound)]\n",
        "print(\"Potential outliers in 'Numeric_Column_1':\")\n",
        "print(outliers)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Removing outliers (example: removing rows where 'Numeric_Column_1' is an outlier)\n",
        "df = df[~((df['Numeric_Column_1'] < lower_bound) | (df['Numeric_Column_1'] > upper_bound))]\n",
        "print(\"DataFrame after removing outliers:\")\n",
        "print(df)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "5c7rneWaNT7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type conversion (if necessary)\n",
        "# Check current data types\n",
        "print(\"Current data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Example: converting categorical columns to 'category' dtype for memory efficiency\n",
        "df['Categorical_Column_1'] = df['Categorical_Column_1'].astype('category')\n",
        "df['Categorical_Column_2'] = df['Categorical_Column_2'].astype('category')\n",
        "\n",
        "print(\"Data types after conversion:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display the final cleaned DataFrame info\n",
        "print(\"Final cleaned DataFrame info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "yY016GiBMmHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalization and Scaling**"
      ],
      "metadata": {
        "id": "JF3k6l3deIny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select numerical columns for scaling\n",
        "numerical_cols = ['Numeric_Column_1', 'Numeric_Column_2']\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df[numerical_cols] = scaler_minmax.fit_transform(df[numerical_cols])\n",
        "\n",
        "print(\"DataFrame after Min-Max Scaling:\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wnGP90sROuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numerical columns for scaling\n",
        "numerical_cols = ['Numeric_Column_1', 'Numeric_Column_2']\n",
        "\n",
        "# Standard Scaling (Z-score normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "df[numerical_cols] = scaler_standard.fit_transform(df[numerical_cols])\n",
        "\n",
        "print(\"DataFrame after Standard Scaling:\")\n",
        "print(df)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "FcRMbrujRUFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformation**"
      ],
      "metadata": {
        "id": "tCe5WjnleRgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
        "\n",
        "# Select a numerical column for transformation\n",
        "column_to_transform = 'Numeric_Column_1'\n",
        "\n",
        "# Plot histogram before transformation\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df[column_to_transform], bins=15, edgecolor='black')\n",
        "plt.title(f'Histogram of {column_to_transform} Before Transformation')\n",
        "plt.xlabel(column_to_transform)\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Quantile Transformation\n",
        "quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
        "df[f'{column_to_transform}_quantile'] = quantile_transformer.fit_transform(df[[column_to_transform]])\n",
        "\n",
        "# Plot histogram after Quantile Transformation\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(df[f'{column_to_transform}_quantile'], bins=15, edgecolor='black')\n",
        "plt.title(f'Histogram After Quantile Transformation')\n",
        "plt.xlabel(f'{column_to_transform}_quantile')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "# Power Transformation (Yeo-Johnson or Box-Cox)\n",
        "# Yeo-Johnson works with positive and negative values\n",
        "power_transformer = PowerTransformer(method='yeo-johnson')\n",
        "df[f'{column_to_transform}_power'] = power_transformer.fit_transform(df[[column_to_transform]])\n",
        "\n",
        "# Plot histogram after Power Transformation\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(df[f'{column_to_transform}_power'], bins=15, edgecolor='black')\n",
        "plt.title(f'Histogram After Power Transformation')\n",
        "plt.xlabel(f'{column_to_transform}_power')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDataFrame with transformed columns:\")\n",
        "print(df[[column_to_transform, f'{column_to_transform}_quantile', f'{column_to_transform}_power']].head())\n"
      ],
      "metadata": {
        "id": "l1Ysiom4PW6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Discretization and Binning**"
      ],
      "metadata": {
        "id": "duoNa4o0ecPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Increase the number of rows for better distribution in binning\n",
        "num_rows = 100\n",
        "\n",
        "# Create a new dictionary with data for the new dataset\n",
        "new_data = {\n",
        "    # A column with a range of values suitable for binning\n",
        "    'Income': np.random.randint(20000, 150000, size=num_rows),\n",
        "    # A column with values that might benefit from polynomial features\n",
        "    'Years_of_Experience': np.random.uniform(0, 25, size=num_rows).round(1),\n",
        "    # Two columns to create interaction features from\n",
        "    'Spend_per_Visit': np.random.uniform(10, 200, size=num_rows).round(2),\n",
        "    'Visits_per_Month': np.random.randint(1, 10, size=num_rows),\n",
        "    # A potential target variable based on some interaction\n",
        "    'Customer_Value': np.random.normal(loc=np.random.uniform(100, 500, size=num_rows), scale=np.random.uniform(20, 100, size=num_rows))\n",
        "}\n",
        "\n",
        "# Create the new DataFrame\n",
        "df_new = pd.DataFrame(new_data)\n"
      ],
      "metadata": {
        "id": "pelbQWzIPKHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "bYKT0VoZPI2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Binning 'Income' into 4 bins (e.g., Low, Medium, High, Very High)\n",
        "# We can use pd.cut for equal-width bins or pd.qcut for equal-frequency bins\n",
        "\n",
        "# Using pd.cut for equal-width bins\n",
        "# Determine the bin edges\n",
        "min_income = df_new['Income'].min()\n",
        "max_income = df_new['Income'].max()\n",
        "# Let's define the bin labels\n",
        "bin_labels = ['Low', 'Medium', 'High', 'Very High']\n",
        "# Create bins with pd.cut\n",
        "df_new['Income_Bins_EqualWidth'] = pd.cut(df_new['Income'], bins=len(bin_labels), labels=bin_labels, include_lowest=True)\n",
        "\n",
        "print(\"\\nDataFrame after Equal-Width Binning on 'Income':\")\n",
        "print(df_new[['Income', 'Income_Bins_EqualWidth']].head())\n",
        "\n",
        "# Using pd.qcut for equal-frequency bins (quantiles)\n",
        "# This will ensure approximately the same number of records in each bin\n",
        "df_new['Income_Bins_EqualFrequency'] = pd.qcut(df_new['Income'], q=len(bin_labels), labels=bin_labels, duplicates='drop')\n",
        "\n",
        "print(\"\\nDataFrame after Equal-Frequency Binning on 'Income':\")\n",
        "print(df_new[['Income', 'Income_Bins_EqualFrequency']].head())\n",
        "\n",
        "# Display the value counts for each binning method\n",
        "print(\"\\nValue counts for Equal-Width Bins:\")\n",
        "print(df_new['Income_Bins_EqualWidth'].value_counts())\n",
        "\n",
        "print(\"\\nValue counts for Equal-Frequency Bins:\")\n",
        "print(df_new['Income_Bins_EqualFrequency'].value_counts())\n",
        "\n",
        "# Optionally, visualize the distribution of the binned column\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "df_new['Income_Bins_EqualWidth'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Income (Equal-Width Bins)')\n",
        "plt.xlabel('Income Bin')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df_new['Income_Bins_EqualFrequency'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Income (Equal-Frequency Bins)')\n",
        "plt.xlabel('Income Bin')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v_XFSPGTTSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "USv4aHdSVy_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply discretization to the 'Income' column using pd.cut\n",
        "# Let's create 5 bins of approximately equal width\n",
        "df_new['Income_Bins_Cut'] = pd.cut(df_new['Income'], bins=5)\n",
        "\n",
        "print(\"\\nDataFrame after Equal-Width Binning on 'Income' with 5 bins:\")\n",
        "print(df_new[['Income', 'Income_Bins_Cut']].head())\n",
        "\n",
        "# Apply discretization to the 'Income' column using pd.qcut\n",
        "# Let's create 5 bins with approximately equal frequencies\n",
        "df_new['Income_Bins_Qcut'] = pd.qcut(df_new['Income'], q=5, duplicates='drop')\n",
        "\n",
        "print(\"\\nDataFrame after Equal-Frequency Binning on 'Income' with 5 bins:\")\n",
        "print(df_new[['Income', 'Income_Bins_Qcut']].head())\n",
        "\n",
        "# Display the value counts for each binning method\n",
        "print(\"\\nValue counts for Equal-Width Bins (5 bins):\")\n",
        "print(df_new['Income_Bins_Cut'].value_counts())\n",
        "\n",
        "print(\"\\nValue counts for Equal-Frequency Bins (5 bins):\")\n",
        "print(df_new['Income_Bins_Qcut'].value_counts())\n",
        "\n",
        "# Optionally, visualize the distribution of the new binned columns\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "df_new['Income_Bins_Cut'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Income (Equal-Width Bins, 5 bins)')\n",
        "plt.xlabel('Income Bin')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df_new['Income_Bins_Qcut'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Income (Equal-Frequency Bins, 5 bins)')\n",
        "plt.xlabel('Income Bin')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJ4We9GDWWnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "qMVrVZSXWiW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Polynomial Interaction**"
      ],
      "metadata": {
        "id": "gmKgJjJTepUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Select the numerical columns for creating polynomial and interaction features\n",
        "# Let's use 'Years_of_Experience' and 'Spend_per_Visit'\n",
        "\n",
        "features_for_poly = ['Years_of_Experience', 'Spend_per_Visit']\n",
        "\n",
        "# Create a PolynomialFeatures object\n",
        "# degree=2 includes the original features, squares of each feature, and the interaction term\n",
        "# include_bias=False means we don't add a column of all ones (which is typically handled by the model's intercept)\n",
        "poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Fit and transform the selected columns\n",
        "poly_features = poly_transformer.fit_transform(df_new[features_for_poly])\n",
        "\n",
        "# The get_feature_names_out method helps understand the new feature names\n",
        "poly_feature_names = poly_transformer.get_feature_names_out(features_for_poly)\n",
        "\n",
        "# Create a new DataFrame from the polynomial features\n",
        "df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_new.index)\n",
        "\n",
        "# Add these new polynomial and interaction features back to the original DataFrame\n",
        "df_new = pd.concat([df_new, df_poly], axis=1)\n",
        "\n",
        "print(\"\\nDataFrame after adding Polynomial and Interaction Features:\")\n",
        "print(df_new.head())\n",
        "\n",
        "# You can see new columns like 'Years_of_Experience^2', 'Spend_per_Visit^2', and 'Years_of_Experience Spend_per_Visit'"
      ],
      "metadata": {
        "id": "woNY0hBhXE_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "mYjt3yOpXTwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoding Techniques**"
      ],
      "metadata": {
        "id": "ZU5lwUUfeyyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Create a new sample DataFrame for demonstrating encoding\n",
        "encoding_data = {\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Green', 'Blue', 'Red', 'Green', 'Blue', 'Red'],\n",
        "    'Size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small'],\n",
        "    'Rating': ['A', 'B', 'C', 'A', 'C', 'B', 'A', 'C', 'B', 'A']\n",
        "}\n",
        "df_encode = pd.DataFrame(encoding_data)\n",
        "\n",
        "print(\"Original DataFrame for Encoding:\")\n",
        "print(df_encode)\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Label Encoding ---\n",
        "print(\"--- Label Encoding ---\")\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply Label Encoding to the 'Size' column\n",
        "df_encode['Size_Encoded'] = label_encoder.fit_transform(df_encode['Size'])\n",
        "\n",
        "print(\"DataFrame after Label Encoding 'Size':\")\n",
        "print(df_encode)\n",
        "print(\"\\n\")\n",
        "\n",
        "# To see the mapping between original labels and encoded values\n",
        "print(\"Label Encoding Mapping for 'Size':\")\n",
        "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dgXnvAnpYIbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_data = {\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Green', 'Blue', 'Red', 'Green', 'Blue', 'Red'],\n",
        "    'Size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small'],\n",
        "    'Rating': ['A', 'B', 'C', 'A', 'C', 'B', 'A', 'C', 'B', 'A']\n",
        "}\n",
        "df_encode = pd.DataFrame(encoding_data)\n",
        "\n",
        "\n",
        "# Create a OneHotEncoder object\n",
        "# sparse=False returns a dense NumPy array instead of a sparse matrix\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Reshape the column to be a 2D array because fit_transform expects it\n",
        "onehot_encoded_colors = onehot_encoder.fit_transform(df_encode[['Color']])\n",
        "\n",
        "# The get_feature_names_out method helps understand the new feature names\n",
        "onehot_feature_names = onehot_encoder.get_feature_names_out(['Color'])\n",
        "\n",
        "# Create a new DataFrame from the one-hot encoded features\n",
        "df_onehot_colors = pd.DataFrame(onehot_encoded_colors, columns=onehot_feature_names, index=df_encode.index)\n",
        "\n",
        "# Concatenate the original DataFrame with the one-hot encoded columns\n",
        "df_encode = pd.concat([df_encode, df_onehot_colors], axis=1)\n",
        "\n",
        "print(\"DataFrame after One-Hot Encoding 'Color':\")\n",
        "print(df_encode)\n",
        "print(\"\\n\")\n",
        "\n",
        "# You can perform One-Hot Encoding directly using pandas get_dummies, which is often simpler\n",
        "print(\"--- One-Hot Encoding using pandas get_dummies ---\")\n",
        "\n",
        "# Select the categorical column(s) to encode\n",
        "columns_to_onehot = ['Rating']\n",
        "\n",
        "# Apply get_dummies\n",
        "df_onehot_pd = pd.get_dummies(df_encode[columns_to_onehot], prefix=columns_to_onehot)\n",
        "\n",
        "# Concatenate the original DataFrame with the new one-hot encoded columns from pandas\n",
        "df_encode = pd.concat([df_encode, df_onehot_pd], axis=1)\n",
        "\n",
        "# Optionally, drop the original categorical columns if you no longer need them\n",
        "# df_encode = df_encode.drop(columns=columns_to_onehot + ['Color']) # also drop the 'Color' column\n",
        "\n",
        "print(\"DataFrame after One-Hot Encoding 'Rating' using pandas get_dummies:\")\n",
        "print(df_encode)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Final DataFrame with both Label and One-Hot Encoding:\")\n",
        "df_encode"
      ],
      "metadata": {
        "id": "KiqpjO-pbA-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Reduction**"
      ],
      "metadata": {
        "id": "NA2vhJcCe9jr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Ubg7wD3Tg2"
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwTtHJFW3Tg9"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "print(iris.data)\n",
        "print(iris.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ibas_63ThB"
      },
      "source": [
        "print(iris.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: concatenate data and target vertically\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming iris.data and iris.target are already loaded as in the previous example\n",
        "concatenated_data = np.concatenate((iris.data, iris.target.reshape(-1,1)), axis=1)\n",
        "print(concatenated_data)\n"
      ],
      "metadata": {
        "id": "4MQsYAkFus98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(concatenated_data)\n",
        "df"
      ],
      "metadata": {
        "id": "WTvEE347vcXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwc8UXOs3ThF"
      },
      "source": [
        "## Components of dimensionality reduction\n",
        "- Feature selection - select a subset\n",
        "- Feature extraction - build a new feature from original feature set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "gOCWABh4vyNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE, SelectFromModel\n",
        "\n",
        "# Assuming df is your DataFrame and the target variable is the last column (index 4 for iris data)\n",
        "# Separate features (X) and target (y)\n",
        "X = df.iloc[:, :-1] # All columns except the last one\n",
        "y = df.iloc[:, -1]  # The last column is the target\n",
        "\n",
        "# --- Filter Method: SelectKBest ---\n",
        "print(\"--- Filter Method: SelectKBest ---\")\n",
        "\n",
        "# Using chi2 for non-negative data like counts (often used with categorical features)\n",
        "# Since iris data is numerical, f_classif is more appropriate for classification\n",
        "# Let's use f_classif (ANOVA F-value) to select features based on their relationship with the target\n",
        "# Select the top k features, e.g., k=2\n",
        "k = 2\n",
        "selector_fclassif = SelectKBest(score_func=f_classif, k=k)\n",
        "\n",
        "# Fit the selector on the data and transform it\n",
        "X_filtered_fclassif = selector_fclassif.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices_fclassif = selector_fclassif.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features (assuming original column names are available)\n",
        "# If not available, you can use the indices\n",
        "selected_feature_names_fclassif = X.columns[selected_feature_indices_fclassif]\n",
        "\n",
        "print(f\"Selected features using SelectKBest (f_classif, k={k}): {selected_feature_names_fclassif.tolist()}\")\n",
        "print(\"Transformed data (filtered):\")\n",
        "print(X_filtered_fclassif[:5]) # Display the first 5 rows\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "tMq0B5usbfq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using mutual_info_classif\n",
        "selector_mutual_info = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "X_filtered_mutual_info = selector_mutual_info.fit_transform(X, y)\n",
        "selected_feature_indices_mutual_info = selector_mutual_info.get_support(indices=True)\n",
        "selected_feature_names_mutual_info = X.columns[selected_feature_indices_mutual_info]\n",
        "\n",
        "print(f\"Selected features using SelectKBest (mutual_info_classif, k={k}): {selected_feature_names_mutual_info.tolist()}\")\n",
        "print(\"Transformed data (filtered):\")\n",
        "print(X_filtered_mutual_info[:5])\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "atb9rBGEcRrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Wrapper Method: Recursive Feature Elimination (RFE) ---\n",
        "print(\"--- Wrapper Method: Recursive Feature Elimination (RFE) ---\")\n",
        "\n",
        "# RFE uses an estimator (like a model) to rank features and recursively eliminate the weakest ones.\n",
        "# Let's use a RandomForestClassifier as the estimator.\n",
        "estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create the RFE object\n",
        "# n_features_to_select: the number of features to select (can be an integer or float representing a fraction)\n",
        "# step: the number of features to remove at each step (can be an integer or float representing a fraction)\n",
        "n_features_rfe = 2\n",
        "rfe_selector = RFE(estimator=estimator, n_features_to_select=n_features_rfe, step=1)\n",
        "\n",
        "# Fit RFE to the data\n",
        "rfe_selector.fit(X, y)\n",
        "\n",
        "# Get the selected features\n",
        "selected_feature_indices_rfe = rfe_selector.get_support(indices=True)\n",
        "selected_feature_names_rfe = X.columns[selected_feature_indices_rfe]\n",
        "\n",
        "print(f\"Selected features using RFE ({estimator.__class__.__name__}, n_features_to_select={n_features_rfe}): {selected_feature_names_rfe.tolist()}\")\n",
        "\n",
        "# Transform the data to keep only the selected features\n",
        "X_wrapped_rfe = rfe_selector.transform(X)\n",
        "print(\"Transformed data (wrapped - RFE):\")\n",
        "print(X_wrapped_rfe[:5])\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "oDXb7SDHcTpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Wrapper Method: SelectFromModel ---\n",
        "print(\"--- Wrapper Method: SelectFromModel ---\")\n",
        "\n",
        "# Selects features based on importance weights from an estimator.\n",
        "# Let's use a RandomForestClassifier again.\n",
        "estimator_sfm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create the SelectFromModel object\n",
        "# threshold: the threshold for the importance scores. Features with scores >= threshold are kept.\n",
        "# You can set threshold='median' or 'mean' or a specific value.\n",
        "sfm_selector = SelectFromModel(estimator=estimator_sfm, threshold='median')\n",
        "\n",
        "# Fit the selector and transform the data\n",
        "sfm_selector.fit(X, y)\n",
        "\n",
        "# Get the selected features\n",
        "selected_feature_indices_sfm = sfm_selector.get_support(indices=True)\n",
        "selected_feature_names_sfm = X.columns[selected_feature_indices_sfm]\n",
        "\n",
        "print(f\"Selected features using SelectFromModel ({estimator_sfm.__class__.__name__}, threshold='median'): {selected_feature_names_sfm.tolist()}\")\n",
        "\n",
        "# Transform the data to keep only the selected features\n",
        "X_wrapped_sfm = sfm_selector.transform(X)\n",
        "print(\"Transformed data (wrapped - SelectFromModel):\")\n",
        "print(X_wrapped_sfm[:5])\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "TXQ8pFp8cGyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Hybrid Method ---\n",
        "print(\"--- Hybrid Method ---\")\n",
        "\n",
        "# Hybrid methods combine aspects of filter and wrapper methods.\n",
        "# A common approach is to use a filter method to reduce the search space,\n",
        "# then apply a wrapper or embedded method on the reduced set.\n",
        "# Or, use an embedded method's feature importances within a wrapper framework.\n",
        "\n",
        "# Example Hybrid Approach: Filter + Wrapper (RFE)\n",
        "# 1. Use SelectKBest as a filter to select the top 'm' features (m > n_features_to_select for RFE)\n",
        "m = 3 # Select top 3 features using a filter\n",
        "filter_step_selector = SelectKBest(score_func=f_classif, k=m)\n",
        "X_filtered_step = filter_step_selector.fit_transform(X, y)\n",
        "selected_feature_indices_filter_step = filter_step_selector.get_support(indices=True)\n",
        "selected_feature_names_filter_step = X.columns[selected_feature_indices_filter_step]\n",
        "\n",
        "print(f\"Hybrid Step 1 (Filter): Selected {m} features using SelectKBest (f_classif): {selected_feature_names_filter_step.tolist()}\")\n",
        "\n",
        "# Create a DataFrame with the filtered features for the next step\n",
        "X_filtered_df = X[selected_feature_names_filter_step]\n",
        "\n",
        "# 2. Apply RFE on the filtered set to select the final 'n' features (n < m)\n",
        "n_features_hybrid = 2 # Select final 2 features using RFE\n",
        "hybrid_rfe_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "hybrid_rfe_selector = RFE(estimator=hybrid_rfe_estimator, n_features_to_select=n_features_hybrid, step=1)\n",
        "\n",
        "# Fit RFE on the filtered data\n",
        "hybrid_rfe_selector.fit(X_filtered_df, y)\n",
        "\n",
        "# Get the selected features from the filtered set\n",
        "selected_feature_indices_hybrid = hybrid_rfe_selector.get_support(indices=True)\n",
        "\n",
        "# Map the indices back to the original feature names from the filtered set\n",
        "selected_feature_names_hybrid = X_filtered_df.columns[selected_feature_indices_hybrid]\n",
        "\n",
        "print(f\"Hybrid Step 2 (Wrapper): Selected {n_features_hybrid} final features using RFE: {selected_feature_names_hybrid.tolist()}\")\n",
        "\n",
        "# Transform the original data to keep only the final selected features from the hybrid method\n",
        "X_hybrid = X[selected_feature_names_hybrid]\n",
        "print(\"Transformed data (hybrid - Filter + RFE):\")\n",
        "print(X_hybrid[:5])\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Another Hybrid Approach: Embedded + Wrapper (SelectFromModel with a threshold based on importance)\n",
        "# 1. Use an embedded method (e.g., RandomForest feature importances) to get feature importances.\n",
        "# The SelectFromModel method already does this internally when fit is called.\n",
        "\n",
        "# 2. Use SelectFromModel with a threshold (implicitly uses feature importances).\n",
        "# We already demonstrated this in the wrapper section. The strength of SelectFromModel\n",
        "# comes from using an embedded method's importance scores.\n",
        "\n",
        "# So, the SelectFromModel example can also be considered a form of hybrid method if\n",
        "# the underlying estimator is an embedded method that provides feature importances\n",
        "# (like tree-based models or models with L1 regularization).\n",
        "\n",
        "print(\"SelectFromModel using RandomForest (an embedded method) can be considered a hybrid approach:\")\n",
        "print(f\"Selected features using SelectFromModel ({estimator_sfm.__class__.__name__}, threshold='median'): {selected_feature_names_sfm.tolist()}\")\n",
        "print(\"Transformed data (hybrid/wrapped - SelectFromModel):\")\n",
        "print(X_wrapped_sfm[:5])\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "rKOgmHrUc8aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptpqb7gQ3ThI"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV7PBurv3ThJ"
      },
      "source": [
        "X = iris.data\n",
        "Y = iris.target\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN3Lqfw43ThK"
      },
      "source": [
        "pca = PCA(n_components=2) #unsupervised\n",
        "X_R = pca.fit(X).transform(X)\n",
        "print(X_R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMOzXWxh3ThM"
      },
      "source": [
        "lda = LDA(n_components=2) #converting 4 dimension to 2. It is a supervised\n",
        "X_L = lda.fit(X,Y).transform(X)\n",
        "print(X_L)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}